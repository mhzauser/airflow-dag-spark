# Example Airflow Dag with Spark Submit and Spark Simple Test

this project helps you to create first airflow dag

## Description
We Setup Data platform Includes (Apache Spark Cluster, Apache Airflow (Development and Production), Apache Superset)
and we worked on several Data projects and our challange was that we wanted to work on projects separated and wanted to workflow for read this journey read this article [commit change]()
In this repository  I wanted to write the sample dag to help others in this workflow.
let's get start! 


### Dependencies
you have to install this dependencies on airflow machine:
```
pip install -r requirements.txt
```

## Getting Started
After Implementing the Infrastructure in your company or setup the environment on your machine
* clone the project to AIRFLOW_PATH/dags and airflow automatically find the dag 
* for gitlab
    * setup you repository
    * create dev branch for airflow development mode 
    * set tag for example v0.0.1release or v0.0.1dev 
    * gitlab runners worked and send code to airflow 

## Authors

Contributors names and contact info

Call me Megahertz

Email : [mhzuser96@gmail.com](mhzuser96@gmail.com)

## Acknowledgments

Apache Airflow, Apache Spark
* [Apache Airflow](https://airflow.apache.org)
* [Apache Spark](https://spark.apache.org)
